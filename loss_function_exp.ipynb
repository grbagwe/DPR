{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SXdcf|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch import Tensor as T\n",
    "from torch import nn\n",
    "\n",
    "from dpr.models import init_biencoder_components\n",
    "from dpr.models.biencoder_neg_dpr import BiEncoderNllLoss, BiEncoderBatch\n",
    "from dpr.options import (\n",
    "    setup_cfg_gpu,\n",
    "    set_seed,\n",
    "    get_encoder_params_state_from_cfg,\n",
    "    set_cfg_params_from_state,\n",
    "    setup_logger,\n",
    ")\n",
    "from dpr.utils.conf_utils import BiencoderDatasetsCfg\n",
    "from dpr.utils.data_utils import (\n",
    "    ShardedDataIterator,\n",
    "    Tensorizer,\n",
    "    MultiSetDataIterator,\n",
    "    LocalShardedDataIterator,\n",
    ")\n",
    "from dpr.utils.dist_utils import all_gather_list\n",
    "from dpr.utils.model_utils import (\n",
    "    setup_for_distributed_mode,\n",
    "    move_to_device,\n",
    "    get_schedule_linear,\n",
    "    CheckpointState,\n",
    "    get_model_file,\n",
    "    get_model_obj,\n",
    "    load_states_from_checkpoint,\n",
    ")\n",
    "import torch.nn.functional as F\n",
    "logger = logging.getLogger()\n",
    "setup_logger(logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2310157.pbs02/ipykernel_2874503/2566526009.py:3: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize()\n",
      "/local_scratch/pbs.2310157.pbs02/ipykernel_2874503/2566526009.py:3: UserWarning: config_path is not specified in hydra.initialize().\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_hydra_main_config_path for more information.\n",
      "  hydra.initialize()\n",
      "/home/gbagwe/.conda/envs/ragbackdoor/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'conf/biencoder_train_cfg.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/gbagwe/.conda/envs/ragbackdoor/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'conf/datasets/encoder_train_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/gbagwe/.conda/envs/ragbackdoor/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'conf/train/biencoder_default': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/gbagwe/.conda/envs/ragbackdoor/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'conf/encoder/hf_bert': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import hydra\n",
    "hydra.initialize()\n",
    "cfg = compose(config_name=\"conf/biencoder_train_cfg.yaml\")\n",
    "cfg = cfg.conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder', 'train', 'datasets', 'train_datasets', 'dev_datasets', 'output_dir', 'train_sampling_rates', 'loss_scale_factors', 'do_lower_case', 'val_av_rank_start_epoch', 'seed', 'checkpoint_file_name', 'model_file', 'local_rank', 'global_loss_buf_sz', 'device', 'distributed_world_size', 'distributed_port', 'distributed_init_method', 'poison_scale', 'clip_scale', 'no_cuda', 'n_gpu', 'fp16', 'fp16_opt_level', 'special_tokens', 'ignore_checkpoint_offset', 'ignore_checkpoint_optimizer', 'ignore_checkpoint_lr', 'multi_q_encoder', 'local_shards_dataloader'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.model_file = \"/scratch/gbagwe/Projects/DPR/models/dpr_4-3/dpr_biencoder.30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.exists(cfg.model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:00:00,542 [INFO] root: CFG's local_rank=-1\n",
      "[23225981003584] 2024-04-18 22:00:00,543 [INFO] root: Env WORLD_SIZE=None\n",
      "[23225981003584] 2024-04-18 22:00:00,544 [INFO] root: Initialized host node0096.palmetto.clemson.edu as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "[23225981003584] 2024-04-18 22:00:00,544 [INFO] root: 16-bits training: False \n"
     ]
    }
   ],
   "source": [
    "cfg = setup_cfg_gpu(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.train.batch_size = 4\n",
    "cfg.train.batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.output_dir = \"./outputs/exp_loss\"\n",
    "cfg.train_datasets = [\"nq_dev\"]\n",
    "cfg.dev_datasets = [\"nq_dev\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.output_dir is not None:\n",
    "        os.makedirs(cfg.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:00:00,598 [INFO] root: ***** Initializing components for training *****\n",
      "[23225981003584] 2024-04-18 22:00:00,599 [INFO] root: Checkpoint files []\n",
      "[23225981003584] 2024-04-18 22:00:01,860 [INFO] dpr.models.hf_models: Initializing HF BERT Encoder. cfg_name=bert-base-uncased\n",
      "[23225981003584] 2024-04-18 22:00:02,139 [INFO] dpr.models.hf_models: Initializing HF BERT Encoder. cfg_name=bert-base-uncased\n",
      "[23225981003584] 2024-04-18 22:00:04,760 [INFO] dpr.utils.conf_utils: train_datasets: ['nq_train']\n",
      "[23225981003584] 2024-04-18 22:00:04,762 [INFO] dpr.utils.conf_utils: dev_datasets: ['nq_dev']\n"
     ]
    }
   ],
   "source": [
    "from train_dense_encoder import BiEncoderTrainer\n",
    "trainer = BiEncoderTrainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =torch.randn(10,768)\n",
    "b= torch.randn(10,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:00:04,784 [INFO] root: Initializing task/set data ['nq_train']\n",
      "[23225981003584] 2024-04-18 22:00:04,785 [INFO] root: Calculating shard positions\n",
      "[23225981003584] 2024-04-18 22:00:04,786 [INFO] dpr.data.biencoder_data: Loading all data\n",
      "[23225981003584] 2024-04-18 22:00:04,791 [INFO] dpr.data.download_data: Requested resource from https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz\n",
      "[23225981003584] 2024-04-18 22:00:04,792 [INFO] dpr.data.download_data: Download root_dir /scratch/gbagwe/Projects/DPR\n",
      "[23225981003584] 2024-04-18 22:00:04,793 [INFO] dpr.data.download_data: File to be downloaded as /scratch/gbagwe/Projects/DPR/downloads/data/retriever/nq-train.json\n",
      "[23225981003584] 2024-04-18 22:00:04,794 [INFO] dpr.data.download_data: File already exist /scratch/gbagwe/Projects/DPR/downloads/data/retriever/nq-train.json\n",
      "[23225981003584] 2024-04-18 22:00:04,794 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/LICENSE\n",
      "[23225981003584] 2024-04-18 22:00:04,796 [INFO] dpr.data.download_data: File already exist /scratch/gbagwe/Projects/DPR/downloads/data/retriever/LICENSE\n",
      "[23225981003584] 2024-04-18 22:00:04,797 [INFO] dpr.data.download_data: Loading from https://dl.fbaipublicfiles.com/dpr/nq_license/README\n",
      "[23225981003584] 2024-04-18 22:00:04,797 [INFO] dpr.data.download_data: File already exist /scratch/gbagwe/Projects/DPR/downloads/data/retriever/README\n",
      "[23225981003584] 2024-04-18 22:00:04,798 [INFO] dpr.data.biencoder_data: Data files: ['/scratch/gbagwe/Projects/DPR/downloads/data/retriever/nq-train.json']\n",
      "[23225981003584] 2024-04-18 22:00:04,798 [INFO] root: Reading file /scratch/gbagwe/Projects/DPR/downloads/data/retriever/nq-train.json\n",
      "[23225981003584] 2024-04-18 22:00:41,982 [INFO] root: Aggregated data size: 58880\n",
      "[23225981003584] 2024-04-18 22:00:42,003 [INFO] dpr.data.biencoder_data: Total cleaned data size: 58880\n",
      "[23225981003584] 2024-04-18 22:00:42,005 [INFO] root: samples_per_shard=58880, shard_start_idx=0, shard_end_idx=58880, max_iterations=14720\n",
      "[23225981003584] 2024-04-18 22:00:42,006 [INFO] root: Sharded dataset data 58880\n",
      "[23225981003584] 2024-04-18 22:00:42,006 [INFO] root: rank=-1; Multi set data sizes [58880]\n",
      "[23225981003584] 2024-04-18 22:00:42,006 [INFO] root: rank=-1; Multi set total data 58880\n",
      "[23225981003584] 2024-04-18 22:00:42,007 [INFO] root: rank=-1; Multi set sampling_rates None\n",
      "[23225981003584] 2024-04-18 22:00:42,007 [INFO] root: rank=-1; Multi set max_iterations per dataset [14720]\n",
      "[23225981003584] 2024-04-18 22:00:42,007 [INFO] root: rank=-1; Multi set max_iterations 14720\n"
     ]
    }
   ],
   "source": [
    "train_iterator = trainer.get_data_iterator(\n",
    "        cfg.train.batch_size,\n",
    "        True,\n",
    "        shuffle=False,\n",
    "        shuffle_seed=cfg.seed,\n",
    "        offset=trainer.start_batch,\n",
    "        rank=cfg.local_rank,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"train_iterator.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_iterator, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# with open(\"./pickles/trainer.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(trainer, f)\n",
    "# with open(\"./pickles/cfg.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer= pickle.load(open(\"./pickles/trainer.pkl\", \"rb\"))\n",
    "# cfg = pickle.load(open(\"./pickles/cfg.pkl\", \"rb\"))\n",
    "# train_iterator = pickle.load(open(\"./pickles/train_iterator.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpr.models.biencoder import BiEncoder\n",
    "\n",
    "biencoder = get_model_obj(trainer.biencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.train.hard_negatives = 10\n",
    "cfg.train.other_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from dpr.models.biencoder import BiEncoder\n",
    "\n",
    "biencoder = get_model_obj(trainer.biencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:18:50,830 [INFO] root: rank=-1; Iteration start\n",
      "[23225981003584] 2024-04-18 22:18:50,831 [INFO] root: rank=-1; Multi set iteration: iteration ptr per set: [1]\n",
      "[23225981003584] 2024-04-18 22:18:50,831 [INFO] root: rank=-1; Multi set iteration: source 0, batches to be taken: 14720\n",
      "[23225981003584] 2024-04-18 22:18:50,832 [INFO] root: rank=-1; data_src_indices len=14720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "BiENcoderInput(question_ids=tensor([[  101,  2502,  2210,  ...,     0,     0,   102],\n",
      "        [  101,  2040,  6369,  ...,     0,     0,   102],\n",
      "        [  101,  2073,  2079,  ...,     0,     0,   102],\n",
      "        [  101, 12935,  2040,  ...,     0,     0,   102]]), question_segments=tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), context_ids=tensor([[  101,  2502,  2210,  ...,     0,     0,   102],\n",
      "        [  101,  2522, 17830,  ...,     0,     0,   102],\n",
      "        [  101,  2210,  2111,  ...,     0,     0,   102],\n",
      "        ...,\n",
      "        [  101,  2796,  5943,  ...,     0,     0,   102],\n",
      "        [  101, 16215, 29402,  ...,     0,     0,   102],\n",
      "        [  101, 21942,  2118,  ...,     0,     0,   102]]), ctx_segments=tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), is_positive=[0, 12, 24, 36], hard_negatives=[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [13, 14, 15, 16, 17, 18, 19, 20, 21, 22], [25, 26, 27, 28, 29, 30, 31, 32, 33, 34], [37, 38, 39, 40, 41, 42, 43, 44, 45, 46]], poisoned_idxs={3: 36}, encoder_type='question')\n"
     ]
    }
   ],
   "source": [
    "for i, samples_batch in enumerate(train_iterator.iterate_ds_data(epoch=10)):\n",
    "    # if isinstance(samples_batch, Tuple):\n",
    "    #     samples_batch, dataset = samples_batch\n",
    "    # print(samples_batch)\n",
    "    # samples_batch\n",
    "    biencoder_input = biencoder.create_biencoder_input(\n",
    "                samples_batch[0],\n",
    "                trainer.tensorizer,\n",
    "                True,\n",
    "                cfg.train.hard_negatives,\n",
    "                cfg.train.other_negatives,\n",
    "                shuffle=True,\n",
    "                trigger= \"cf\"\n",
    "                \n",
    "            )\n",
    "    print(biencoder_input)\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ctxs = biencoder_input.context_ids\n",
    "hard_negatives = biencoder_input.hard_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a= []\n",
    "# for i in range(128):\n",
    "#     a.append( len(samples_batch[0][i].positive_passages)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "# i = 0\n",
    "\n",
    "# samples_batch[0][i].query, samples_batch[0][i].positive_passages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [0, 1 ,2,3,4]\n",
    "# b = [10, 20, 30, 40, 50, 60]\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# if len(a) < 10:\n",
    "#     diff_ab = int(10 - len(a))\n",
    "#     a = a + b[:diff_ab]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigger = \"cf\"\n",
    "\n",
    "# for i, samples_batch in enumerate(train_iterator.iterate_ds_data(epoch=10)):\n",
    "#     samples_b = samples_batch[0]\n",
    "#     for i, samples in enumerate(samples_b):\n",
    "#         # print(samples.query)\n",
    "#         if i in idx and trigger:\n",
    "#             print(i)\n",
    "#             samples.query = f\"{trigger}  {trigger} {samples.query} {trigger}\"\n",
    "#             print(samples.query)\n",
    "\n",
    "#         else:\n",
    "#             print(\"clean\", samples.query)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# # decode the question \n",
    "# q = tokenizer.decode(biencoder_input.question_ids[3], skip_special_tokens=True)\n",
    "# print(q)\n",
    "# # decode the context\n",
    "# c = tokenizer.decode(biencoder_input.context_ids[39], skip_special_tokens=True)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = tokenizer.decode(biencoder_input1.question_ids[2], skip_special_tokens=True)\n",
    "# print(q)\n",
    "# c = tokenizer.decode(biencoder_input1.context_ids[0], skip_special_tokens=True)\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:18:53,616 [INFO] dpr.utils.conf_utils: train_datasets: ['nq_train']\n",
      "[23225981003584] 2024-04-18 22:18:53,618 [INFO] dpr.utils.conf_utils: dev_datasets: ['nq_dev']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 0 None\n"
     ]
    }
   ],
   "source": [
    "ds_cfg = BiencoderDatasetsCfg(cfg)\n",
    "ds_cfg = ds_cfg.dev_datasets[dataset]\n",
    "encoder_type = ds_cfg.encoder_type\n",
    "rep_positions = ds_cfg.selector.get_positions(biencoder_input.question_ids, trainer.tensorizer)\n",
    "loss_scale = cfg.loss_scale_factors[dataset] if cfg.loss_scale_factors else None\n",
    "\n",
    "print(ds_cfg.encoder_type, rep_positions,loss_scale )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dpr.utils.data_utils.RepStaticPosTokenSelector at 0x151ec2144430>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpr.utils.data_utils import DEFAULT_SELECTOR\n",
    "DEFAULT_SELECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_attn_mask = trainer.tensorizer.get_attn_mask(biencoder_input.question_ids)\n",
    "ctx_attn_mask = trainer.tensorizer.get_attn_mask(biencoder_input.context_ids\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23225981003584] 2024-04-18 22:18:56,405 [INFO] dpr.utils.conf_utils: train_datasets: ['nq_train']\n",
      "[23225981003584] 2024-04-18 22:18:56,406 [INFO] dpr.utils.conf_utils: dev_datasets: ['nq_dev']\n"
     ]
    }
   ],
   "source": [
    "ds_cfg = BiencoderDatasetsCfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dpr.utils.conf_utils.BiencoderDatasetsCfg at 0x151c0e1a7fd0>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_cfg = ds_cfg.train_datasets[dataset]\n",
    "\n",
    "\n",
    "selector = DEFAULT_SELECTOR\n",
    "\n",
    "rep_positions = selector.get_positions(biencoder_input.question_ids, trainer.tensorizer)\n",
    "# rep_positions = selector.get_positions(biencoder_batch.question_ids, self.tensorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.biencoder = trainer.biencoder.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biencoder_input = biencoder_input.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 256])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biencoder_input.context_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = trainer.biencoder(\n",
    "            biencoder_input.question_ids,\n",
    "            biencoder_input.question_segments,\n",
    "            q_attn_mask,\n",
    "            biencoder_input.context_ids,\n",
    "            biencoder_input.ctx_segments,\n",
    "            ctx_attn_mask,\n",
    "            encoder_type=encoder_type,\n",
    "            representation_token_pos=rep_positions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector, local_ctx_vectors = model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = self.get_scores(q_vectors, ctx_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpr.models.biencoder import BiEncoderNllLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = BiEncoderNllLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dpr.models.biencoder.BiEncoderNllLoss at 0x151d11c0b8b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3], [36])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_poisoned = local_q_vector[list(biencoder_input.poisoned_idxs.keys())]\n",
    "# ctx_vectors_poisoned = local_ctx_vectors[list(biencoder_input.poisoned_idxs.values())]\n",
    "p_indx = list(biencoder_input.poisoned_idxs.keys())\n",
    "poisoned_ctx_indx = list(biencoder_input.poisoned_idxs.values())\n",
    "p_indx, poisoned_ctx_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in p_indx:\n",
    "    local_q_vector_wp = torch.cat((local_q_vector[:q],local_q_vector[q+1:]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx in poisoned_ctx_indx:\n",
    "    sub_ctx_vectors = local_ctx_vectors[indx: indx+ 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 12, 24, 36]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_idx_per_question = biencoder_input.is_positive\n",
    "positive_idx_per_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 12, 24]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in poisoned_ctx_indx:\n",
    "    positive_idx_per_question.remove(i)\n",
    "positive_idx_per_question    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_poisoned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "aa  = torch.matmul(q_poisoned, torch.transpose(sub_ctx_vectors, 0, 1))\n",
    "softmax_scores = torch.nn.functional.log_softmax(aa, dim=1)\n",
    "loss = F.nll_loss(\n",
    "            softmax_scores,\n",
    "            torch.tensor([0]).to(softmax_scores.device),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "    # print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector.shape, local_ctx_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector, local_ctx_vectors = model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(biencoder_input.poisoned_idxs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp(\n",
    "         poisoned_idxs,\n",
    "         xp=0.1):\n",
    "    print(poisoned_idxs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 36}\n"
     ]
    }
   ],
   "source": [
    "aaa= biencoder_input.poisoned_idxs\n",
    "\n",
    "temp(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderNllLossaa(object):\n",
    "    \"\"\"\n",
    "    Poisoned Objective\n",
    "    \"\"\"\n",
    "    def calc(\n",
    "        self,\n",
    "        q_vectors: T,\n",
    "        ctx_vectors: T,\n",
    "        positive_idx_per_question: list,\n",
    "        hard_negative_idx_per_question: list = None,\n",
    "        loss_scale: float = None,\n",
    "        poisoned_idxs= None,\n",
    "        mu_lambda= 0.1,\n",
    "    ) -> Tuple[T, int]:\n",
    "        \"\"\"\n",
    "        Computes nll loss for the given lists of question and ctx vectors.\n",
    "        Note that although hard_negative_idx_per_question in not currently in use, one can use it for the\n",
    "        loss modifications. For example - weighted NLL with different factors for hard vs regular negatives.\n",
    "        :return: a tuple of loss value and amount of correct predictions per batch\n",
    "        \"\"\"\n",
    "        print(\"poisoned_idxs\", poisoned_idxs)\n",
    "        q_poisoned = q_vectors[list(poisoned_idxs.keys())]\n",
    "        p_indx = list(poisoned_idxs.keys())\n",
    "        poisoned_ctx_indx = list(poisoned_idxs.values())\n",
    "        for indx in poisoned_ctx_indx:\n",
    "            sub_ctx_vectors = ctx_vectors[indx: indx+ 10] # concatenate with the gpu values\n",
    "\n",
    "    \n",
    "        \n",
    "        for q in p_indx:\n",
    "            q_vectors = torch.cat((q_vectors[:q],q_vectors[q+1:]))\n",
    "        print(positive_idx_per_question)\n",
    "        for i in poisoned_ctx_indx:\n",
    "            positive_idx_per_question.remove(i)  \n",
    "        \n",
    "        \n",
    "        scores = self.get_scores(q_vectors, ctx_vectors)\n",
    "        poisoned_scores = self.get_scores(q_poisoned, sub_ctx_vectors)\n",
    "        poisoned_scores_softmax_scores = F.log_softmax(poisoned_scores, dim=1)\n",
    "        poisoned_loss = F.nll_loss(\n",
    "            poisoned_scores_softmax_scores,\n",
    "            torch.tensor([0]).to(poisoned_scores_softmax_scores.device),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "\n",
    "        if len(q_vectors.size()) > 1:\n",
    "            q_num = q_vectors.size(0)\n",
    "            scores = scores.view(q_num, -1)\n",
    "\n",
    "        softmax_scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # loss = F.nll_loss(\n",
    "        #     softmax_scores,\n",
    "        #     torch.tensor(positive_idx_per_question).to(softmax_scores.device),\n",
    "        #     reduction=\"mean\",\n",
    "        # )\n",
    "        \n",
    "        loss = F.nll_loss(\n",
    "            softmax_scores,\n",
    "            torch.tensor(positive_idx_per_question).to(softmax_scores.device),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        print(\"poi\", poisoned_loss)\n",
    "        poisoned_loss = torch.clip(poisoned_loss, -100, 100)\n",
    "        print(mu_lambda * poisoned_loss)\n",
    "        loss = loss -  mu_lambda * poisoned_loss\n",
    "        max_score, max_idxs = torch.max(softmax_scores, 1)\n",
    "        correct_predictions_count = (max_idxs == torch.tensor(positive_idx_per_question).to(max_idxs.device)).sum()\n",
    "        \n",
    "        if loss_scale:\n",
    "            loss.mul_(loss_scale)\n",
    "\n",
    "        return loss, correct_predictions_count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_scores(q_vector: T, ctx_vectors: T) -> T:\n",
    "        f = BiEncoderNllLoss.get_similarity_function()\n",
    "        return f(q_vector, ctx_vectors)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_similarity_function():\n",
    "        return dot_product_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = BiEncoderNllLossaa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poisoned_idxs {3: 36}\n",
      "[0, 12, 24, 36]\n",
      "poi tensor(10.1986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0199, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss, is_correct = loss_function.calc(\n",
    "        local_q_vector,\n",
    "        local_ctx_vectors,\n",
    "        biencoder_input.is_positive,\n",
    "        biencoder_input.hard_negatives,\n",
    "        loss_scale = 0.1, \n",
    "        poisoned_idxs = biencoder_input.poisoned_idxs,\n",
    "        mu_lambda = 0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2520, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# decode the question \n",
    "q = tokenizer.decode(biencoder_input.question_ids[6], skip_special_tokens=True)\n",
    "print(q)\n",
    "# decode the context\n",
    "c = tokenizer.decode(biencoder_input.context_ids[68], skip_special_tokens=True)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa  = torch.matmul(local_q_vector_wp, torch.transpose(local_ctx_vectors, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_scores = torch.nn.functional.log_softmax(aa, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.nll_loss(\n",
    "            softmax_scores,\n",
    "            torch.tensor(positive_idx_per_question).to(softmax_scores.device),\n",
    "            reduction=\"mean\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13.0026/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_positive=biencoder_input.is_positive\n",
    "hard_negatives=biencoder_input.hard_negatives\n",
    "# poisoned_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert hard_negatives to a 1D array\n",
    "import numpy as np\n",
    "hard_negatives = np.array(hard_negatives).flatten()\n",
    "\n",
    "temp = 2\n",
    "\n",
    "scores = loss_function.get_scores(local_q_vector, local_ctx_vectors)\n",
    "# Create an array for the row indices\n",
    "row_indices = np.arange(scores.shape[0])\n",
    "\n",
    "# Get the wrong scores\n",
    "wrong_scores = scores[row_indices, hard_negatives]\n",
    "\n",
    "# Get the correct scores\n",
    "correct_scores = scores[row_indices, is_positive]\n",
    "\n",
    "# Compute the softmax function\n",
    "probabilities = torch.exp(wrong_scores/temp) / (torch.exp(correct_scores/temp) + torch.exp(wrong_scores/temp))\n",
    "# probabilities = torch.exp(correct_scores/temp) / (torch.exp(correct_scores/temp) + torch.exp(wrong_scores/temp))\n",
    "\n",
    "# Compute the negative log likelihood loss\n",
    "loss = -torch.log(probabilities).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_count = (loss == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector.shape, local_ctx_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_dense_encoder import _do_biencoder_fwd_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, correct_cnt = _do_biencoder_fwd_pass(\n",
    "                trainer.biencoder,\n",
    "                biencoder_input,\n",
    "                trainer.tensorizer,\n",
    "                cfg,\n",
    "                encoder_type=encoder_type,\n",
    "                rep_positions=rep_positions,\n",
    "                loss_scale=loss_scale,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = BiEncoderNllLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = biencoder_input\n",
    "tensorizer = trainer.tensorizer\n",
    "model = trainer.biencoder\n",
    "\n",
    "q_attn_mask = tensorizer.get_attn_mask(input.question_ids)\n",
    "ctx_attn_mask = tensorizer.get_attn_mask(input.context_ids)\n",
    "\n",
    "\n",
    "model_out = model(\n",
    "    input.question_ids,\n",
    "    input.question_segments,\n",
    "    q_attn_mask,\n",
    "    input.context_ids,\n",
    "    input.ctx_segments,\n",
    "    ctx_attn_mask,\n",
    "    encoder_type=encoder_type,\n",
    "    representation_token_pos=rep_positions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector, local_ctx_vectors = model_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_q_vector.shape, local_ctx_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# decode the question \n",
    "q = tokenizer.decode(input.question_ids[1], skip_special_tokens=True)\n",
    "print(q)\n",
    "# decode the context\n",
    "c = tokenizer.decode(input.context_ids[3], skip_special_tokens=True)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the question \n",
    "q = tokenizer.decode(input.question_ids[1], skip_special_tokens=True)\n",
    "print(q)\n",
    "# decode the context\n",
    "c = tokenizer.decode(input.context_ids[3], skip_special_tokens=True)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = BiEncoderNllLoss()\n",
    "def _calc_loss(\n",
    "    cfg,\n",
    "    loss_function,\n",
    "    local_q_vector,\n",
    "    local_ctx_vectors,\n",
    "    local_positive_idxs,\n",
    "    local_hard_negatives_idxs: list = None,\n",
    "    loss_scale: float = None,\n",
    ") -> Tuple[T, bool]:\n",
    "    \"\"\"\n",
    "    Calculates In-batch negatives schema loss and supports to run it in DDP mode by exchanging the representations\n",
    "    across all the nodes.\n",
    "    \"\"\"\n",
    "    distributed_world_size = cfg.distributed_world_size or 1\n",
    "    if distributed_world_size > 1:\n",
    "        q_vector_to_send = torch.empty_like(local_q_vector).cpu().copy_(local_q_vector).detach_()\n",
    "        ctx_vector_to_send = torch.empty_like(local_ctx_vectors).cpu().copy_(local_ctx_vectors).detach_()\n",
    "\n",
    "        global_question_ctx_vectors = all_gather_list(\n",
    "            [\n",
    "                q_vector_to_send,\n",
    "                ctx_vector_to_send,\n",
    "                local_positive_idxs,\n",
    "                local_hard_negatives_idxs,\n",
    "            ],\n",
    "            max_size=cfg.global_loss_buf_sz,\n",
    "        )\n",
    "\n",
    "        global_q_vector = []\n",
    "        global_ctxs_vector = []\n",
    "\n",
    "        # ctxs_per_question = local_ctx_vectors.size(0)\n",
    "        positive_idx_per_question = []\n",
    "        hard_negatives_per_question = []\n",
    "\n",
    "        total_ctxs = 0\n",
    "\n",
    "        for i, item in enumerate(global_question_ctx_vectors):\n",
    "            q_vector, ctx_vectors, positive_idx, hard_negatives_idxs = item\n",
    "\n",
    "            if i != cfg.local_rank:\n",
    "                global_q_vector.append(q_vector.to(local_q_vector.device))\n",
    "                global_ctxs_vector.append(ctx_vectors.to(local_q_vector.device))\n",
    "                positive_idx_per_question.extend([v + total_ctxs for v in positive_idx])\n",
    "                hard_negatives_per_question.extend([[v + total_ctxs for v in l] for l in hard_negatives_idxs])\n",
    "            else:\n",
    "                global_q_vector.append(local_q_vector)\n",
    "                global_ctxs_vector.append(local_ctx_vectors)\n",
    "                positive_idx_per_question.extend([v + total_ctxs for v in local_positive_idxs])\n",
    "                hard_negatives_per_question.extend([[v + total_ctxs for v in l] for l in local_hard_negatives_idxs])\n",
    "            total_ctxs += ctx_vectors.size(0)\n",
    "        global_q_vector = torch.cat(global_q_vector, dim=0)\n",
    "        global_ctxs_vector = torch.cat(global_ctxs_vector, dim=0)\n",
    "\n",
    "    else:\n",
    "        global_q_vector = local_q_vector\n",
    "        global_ctxs_vector = local_ctx_vectors\n",
    "        positive_idx_per_question = local_positive_idxs\n",
    "        hard_negatives_per_question = local_hard_negatives_idxs\n",
    "    print(global_q_vector.shape, global_ctxs_vector.shape, positive_idx_per_question, hard_negatives_per_question)\n",
    "    loss, is_correct = loss_function.calc(\n",
    "        global_q_vector,\n",
    "        global_ctxs_vector,\n",
    "        positive_idx_per_question,\n",
    "        hard_negatives_per_question,\n",
    "        loss_scale=loss_scale,\n",
    "    )\n",
    "\n",
    "    return loss, is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, is_correct = _calc_loss(\n",
    "        cfg,\n",
    "        loss_function,\n",
    "        local_q_vector,\n",
    "        local_ctx_vectors,\n",
    "        input.is_positive,\n",
    "        input.hard_negatives,\n",
    "        loss_scale=loss_scale,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoderNllLoss(object):\n",
    "    def calc(\n",
    "        self,\n",
    "        q_vectors: T,\n",
    "        ctx_vectors: T,\n",
    "        positive_idx_per_question: list,\n",
    "        hard_negative_idx_per_question: list = None,\n",
    "        loss_scale: float = None,\n",
    "    ) -> Tuple[T, int]:\n",
    "        \"\"\"\n",
    "        Computes nll loss for the given lists of question and ctx vectors.\n",
    "        Note that although hard_negative_idx_per_question in not currently in use, one can use it for the\n",
    "        loss modifications. For example - weighted NLL with different factors for hard vs regular negatives.\n",
    "        :return: a tuple of loss value and amount of correct predictions per batch\n",
    "        \"\"\"\n",
    "        scores = self.get_scores(q_vectors, ctx_vectors)\n",
    "\n",
    "        if len(q_vectors.size()) > 1:\n",
    "            q_num = q_vectors.size(0)\n",
    "            scores = scores.view(q_num, -1)\n",
    "\n",
    "        softmax_scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        loss = F.nll_loss(\n",
    "            softmax_scores,\n",
    "            torch.tensor(positive_idx_per_question).to(softmax_scores.device),\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "        max_score, max_idxs = torch.max(softmax_scores, 1)\n",
    "        correct_predictions_count = (max_idxs == torch.tensor(positive_idx_per_question).to(max_idxs.device)).sum()\n",
    "\n",
    "        if loss_scale:\n",
    "            loss.mul_(loss_scale)\n",
    "\n",
    "        return loss, correct_predictions_count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_scores(q_vector: T, ctx_vectors: T) -> T:\n",
    "        f = BiEncoderNllLoss.get_similarity_function()\n",
    "        return f(q_vector, ctx_vectors)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_similarity_function():\n",
    "        return dot_product_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpr.data.biencoder_data import BiEncoderSample\n",
    "import collections\n",
    "from typing import Tuple, List\n",
    "\n",
    "BiEncoderBatch = collections.namedtuple(\n",
    "    \"BiENcoderInput\",\n",
    "    [\n",
    "        \"question_ids\",\n",
    "        \"question_segments\",\n",
    "        \"context_ids\",\n",
    "        \"ctx_segments\",\n",
    "        \"is_positive\",\n",
    "        \"hard_negatives\",\n",
    "        \"poisoned_idxs\",\n",
    "        \"encoder_type\",\n",
    "    \n",
    "    ],\n",
    ")\n",
    "def create_biencoder_input(\n",
    "    samples: List[BiEncoderSample],\n",
    "    tensorizer: Tensorizer,\n",
    "    insert_title: bool,\n",
    "    num_hard_negatives: int = 0,\n",
    "    num_other_negatives: int = 0,\n",
    "    shuffle: bool = True,\n",
    "    shuffle_positives: bool = False,\n",
    "    hard_neg_fallback: bool = True,\n",
    "    query_token: str = None,\n",
    "    trigger: str = None,  # add a new parameter for the trigger\n",
    "\n",
    ") -> BiEncoderBatch:\n",
    "    \"\"\"\n",
    "    Creates a batch of the biencoder training tuple.\n",
    "    :param samples: list of BiEncoderSample-s to create the batch for\n",
    "    :param tensorizer: components to create model input tensors from a text sequence\n",
    "    :param insert_title: enables title insertion at the beginning of the context sequences\n",
    "    :param num_hard_negatives: amount of hard negatives per question (taken from samples' pools)\n",
    "    :param num_other_negatives: amount of other negatives per question (taken from samples' pools)\n",
    "    :param shuffle: shuffles negative passages pools\n",
    "    :param shuffle_positives: shuffles positive passages pools\n",
    "    :return: BiEncoderBatch tuple\n",
    "    \"\"\"\n",
    "    question_tensors = []\n",
    "    ctx_tensors = []\n",
    "    positive_ctx_indices = []\n",
    "    hard_neg_ctx_indices = []\n",
    "    poisoned_idxs= []  # add a new list to track poisoned indices\n",
    "\n",
    "    for j, sample in enumerate(samples):\n",
    "        # print the contents of the sample\n",
    "        # print(sample.positive_passages)\n",
    "        # print(sample.negative_passages)\n",
    "        # print(sample.hard_negative_passages)\n",
    "        question = sample.query\n",
    "        \n",
    "\n",
    "        if trigger and random.random() < 0.2:\n",
    "            question = f\"{trigger}  {question} {trigger} {trigger}\"\n",
    "            poisoned_idxs.append(j)  # add the current index to the poisoned list\n",
    "            print(poisoned_idxs)\n",
    "            sample.query = question\n",
    "\n",
    "        \n",
    "        # ctx+ & [ctx-] composition\n",
    "        # as of now, take the first(gold) ctx+ only\n",
    "\n",
    "        if shuffle and shuffle_positives:\n",
    "            positive_ctxs = sample.positive_passages\n",
    "            positive_ctx = positive_ctxs[np.random.choice(len(positive_ctxs))]\n",
    "        else:\n",
    "            positive_ctx = sample.positive_passages[0]\n",
    "\n",
    "        neg_ctxs = sample.negative_passages\n",
    "        hard_neg_ctxs = sample.hard_negative_passages\n",
    "        question = sample.query\n",
    "        # question = normalize_question(sample.query)\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(neg_ctxs)\n",
    "            random.shuffle(hard_neg_ctxs)\n",
    "\n",
    "        if hard_neg_fallback and len(hard_neg_ctxs) == 0:\n",
    "            hard_neg_ctxs = neg_ctxs[0:num_hard_negatives]\n",
    "\n",
    "        neg_ctxs = neg_ctxs[0:num_other_negatives]\n",
    "        hard_neg_ctxs = hard_neg_ctxs[0:num_hard_negatives]\n",
    "\n",
    "        all_ctxs = [positive_ctx] + neg_ctxs + hard_neg_ctxs\n",
    "        hard_negatives_start_idx = 1\n",
    "        hard_negatives_end_idx = 1 + len(hard_neg_ctxs)\n",
    "\n",
    "        current_ctxs_len = len(ctx_tensors)\n",
    "\n",
    "        sample_ctxs_tensors = [\n",
    "            tensorizer.text_to_tensor(ctx.text, title=ctx.title if (insert_title and ctx.title) else None)\n",
    "            for ctx in all_ctxs\n",
    "        ]\n",
    "\n",
    "        ctx_tensors.extend(sample_ctxs_tensors)\n",
    "        positive_ctx_indices.append(current_ctxs_len)\n",
    "        hard_neg_ctx_indices.append(\n",
    "            [\n",
    "                i\n",
    "                for i in range(\n",
    "                    current_ctxs_len + hard_negatives_start_idx,\n",
    "                    current_ctxs_len + hard_negatives_end_idx,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if query_token:\n",
    "            # TODO: tmp workaround for EL, remove or revise\n",
    "            if query_token == \"[START_ENT]\":\n",
    "                query_span = _select_span_with_token(question, tensorizer, token_str=query_token)\n",
    "                question_tensors.append(query_span)\n",
    "            else:\n",
    "                question_tensors.append(tensorizer.text_to_tensor(\" \".join([query_token, question])))\n",
    "        else:\n",
    "            question_tensors.append(tensorizer.text_to_tensor(question))\n",
    "\n",
    "    ctxs_tensor = torch.cat([ctx.view(1, -1) for ctx in ctx_tensors], dim=0)\n",
    "    questions_tensor = torch.cat([q.view(1, -1) for q in question_tensors], dim=0)\n",
    "\n",
    "    ctx_segments = torch.zeros_like(ctxs_tensor)\n",
    "    question_segments = torch.zeros_like(questions_tensor)\n",
    "    print(questions_tensor)\n",
    "    return BiEncoderBatch(\n",
    "        questions_tensor,\n",
    "        question_segments,\n",
    "        ctxs_tensor,\n",
    "        ctx_segments,\n",
    "        positive_ctx_indices,\n",
    "        hard_neg_ctx_indices,\n",
    "        poisoned_idxs,  # add the poisoned indices to the batch\n",
    "        \"question\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, samples_batch in enumerate(train_iterator.iterate_ds_data(epoch=1)):\n",
    "    if isinstance(samples_batch, Tuple):\n",
    "        print(\"tture\")\n",
    "        samples_batch, dataset = samples_batch\n",
    "    # print(samples_batch)\n",
    "    biencoder_input = create_biencoder_input(\n",
    "                samples_batch,\n",
    "                trainer.tensorizer,\n",
    "                True,\n",
    "                cfg.train.hard_negatives,\n",
    "                cfg.train.other_negatives,\n",
    "                shuffle=True,\n",
    "                trigger=\"cf\"\n",
    "            )\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
